{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579fb1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Write a Python program to read a Hadoop configuration file and display the core components of Hadoop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b10594",
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "\n",
    "# Specify the path to the Hadoop configuration file\n",
    "config_file_path = '/path/to/hadoop/config/file.xml'\n",
    "\n",
    "# Create a ConfigParser object\n",
    "config = configparser.ConfigParser()\n",
    "\n",
    "# Read the Hadoop configuration file\n",
    "config.read(config_file_path)\n",
    "\n",
    "# Get the core components section\n",
    "core_section = config['core']\n",
    "\n",
    "# Extract and display the core components of Hadoop\n",
    "core_components = core_section.get('fs.defaultFS'), core_section.get('hadoop.tmp.dir')\n",
    "print(\"Core Components of Hadoop:\")\n",
    "for component in core_components:\n",
    "    print(\"- \" + component)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b70f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Implement a Python function that calculates the total file size in a Hadoop Distributed File System (HDFS) directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09c87f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdfs import InsecureClient\n",
    "\n",
    "def calculate_directory_size(hdfs_url, hdfs_path):\n",
    "    # Create an HDFS client\n",
    "    client = InsecureClient(hdfs_url)\n",
    "\n",
    "    # Calculate total file size\n",
    "    total_size = 0\n",
    "    for entry in client.list(hdfs_path, status=True):\n",
    "        if entry['type'] == 'FILE':\n",
    "            total_size += entry['length']\n",
    "\n",
    "    return total_size\n",
    "\n",
    "# Example usage\n",
    "hdfs_url = 'http://localhost:50070'  # Replace with your HDFS NameNode URL\n",
    "hdfs_path = '/path/to/hdfs/directory'  # Replace with your HDFS directory path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff75e75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. Create a Python program that extracts and displays the top N most frequent words from a large text file using the MapReduce approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f487b253",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import heapq\n",
    "\n",
    "\n",
    "class TopNWords(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        words = line.strip().lower().split()\n",
    "        for word in words:\n",
    "            yield word, 1\n",
    "\n",
    "    def combiner(self, word, counts):\n",
    "        yield word, sum(counts)\n",
    "\n",
    "    def reducer_init(self):\n",
    "        self.topN = []\n",
    "\n",
    "    def reducer(self, word, counts):\n",
    "        heapq.heappush(self.topN, (sum(counts), word))\n",
    "        if len(self.topN) > N:\n",
    "            heapq.heappop(self.topN)\n",
    "\n",
    "    def reducer_final(self):\n",
    "        self.topN.sort(reverse=True)\n",
    "        for count, word in self.topN:\n",
    "            yield word, count\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper,\n",
    "                   combiner=self.combiner,\n",
    "                   reducer_init=self.reducer_init,\n",
    "                   reducer=self.reducer,\n",
    "                   reducer_final=self.reducer_final)\n",
    "        ]\n",
    "\n",
    "\n",
    "# Example usage\n",
    "N = 10  # Specify the number of top frequent words you want to extract\n",
    "input_file = 'path/to/large_text_file.txt'  # Replace with the path to your large text file\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    TopNWords.run(args=[input_file])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84dc321",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. Write a Python script that checks the health status of the NameNode and DataNodes in a Hadoop cluster using Hadoop's REST API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3fe0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from collections import Counter\n",
    "import heapq\n",
    "\n",
    "\n",
    "class TopNWords(MRJob):\n",
    "\n",
    "    def configure_args(self):\n",
    "        super(TopNWords, self).configure_args()\n",
    "        self.add_passthru_arg('-n', '--top-n', default=10, type=int, help='Number of top frequent words')\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        words = line.strip().lower().split()\n",
    "        for word in words:\n",
    "            yield word, 1\n",
    "\n",
    "    def combiner(self, word, counts):\n",
    "        yield word, sum(counts)\n",
    "\n",
    "    def reducer_init(self):\n",
    "        self.topN = []\n",
    "\n",
    "    def reducer(self, word, counts):\n",
    "        heapq.heappush(self.topN, (sum(counts), word))\n",
    "        if len(self.topN) > self.options.top_n:\n",
    "            heapq.heappop(self.topN)\n",
    "\n",
    "    def reducer_final(self):\n",
    "        self.topN.sort(reverse=True)\n",
    "        for count, word in self.topN:\n",
    "            yield word, count\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper,\n",
    "                   combiner=self.combiner,\n",
    "                   reducer_init=self.reducer_init,\n",
    "                   reducer=self.reducer,\n",
    "                   reducer_final=self.reducer_final)\n",
    "        ]\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == '__main__':\n",
    "    TopNWords.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fff198",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. Develop a Python program that lists all the files and directories in a specific HDFS path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffa2a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.hdfs as hdfs\n",
    "\n",
    "# Specify the HDFS path\n",
    "hdfs_path = '/path/to/hdfs/directory'  # Replace with your HDFS path\n",
    "\n",
    "# Connect to HDFS\n",
    "hdfs_client = hdfs.connect(host='localhost', port=9000)  # Replace with your HDFS NameNode host and port\n",
    "\n",
    "# List files and directories\n",
    "file_info = hdfs_client.ls(hdfs_path)\n",
    "\n",
    "# Print file and directory names\n",
    "print(\"Files and Directories in HDFS Path:\", hdfs_path)\n",
    "for info in file_info:\n",
    "    print(info['name'])\n",
    "\n",
    "# Close HDFS connection\n",
    "hdfs_client.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e49f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. Implement a Python program that analyzes the storage utilization of DataNodes in a Hadoop cluster and identifies the nodes with the highest and lowest storage capacities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8f01b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdfs import InsecureClient\n",
    "\n",
    "# Specify the HDFS URL and path\n",
    "hdfs_url = 'http://localhost:50070'  # Replace with your HDFS NameNode URL\n",
    "hdfs_path = '/path/to/hdfs/directory'  # Replace with your HDFS path\n",
    "\n",
    "# Create an HDFS client\n",
    "client = InsecureClient(hdfs_url)\n",
    "\n",
    "# List files and directories\n",
    "file_info = client.list(hdfs_path, status=True)\n",
    "\n",
    "# Print file and directory names\n",
    "print(\"Files and Directories in HDFS Path:\", hdfs_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9b1775",
   "metadata": {},
   "outputs": [],
   "source": [
    "7. Create a Python script that interacts with YARN's ResourceManager API to submit a Hadoop job, monitor its progress, and retrieve the final output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda9b949",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "# YARN ResourceManager URL\n",
    "resource_manager_url = 'http://<resource-manager-host>:8088'\n",
    "\n",
    "# Submit a Hadoop job\n",
    "job_submission_url = f'{resource_manager_url}/ws/v1/cluster/apps'\n",
    "headers = {'Content-Type': 'application/json'}\n",
    "data = {\n",
    "    'application-id': 'application_123456789_0001',\n",
    "    'application-name': 'MyHadoopJob',\n",
    "    'am-container-spec': {\n",
    "        'commands': {\n",
    "            'command': 'hadoop jar my_job.jar input_path output_path'\n",
    "        }\n",
    "    },\n",
    "    'application-type': 'MAPREDUCE'\n",
    "}\n",
    "response = requests.post(job_submission_url, headers=headers, json=data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3821b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "8. Create a Python script that interacts with YARN's ResourceManager API to submit a Hadoop job, set resource requirements, and track resource usage during job execution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9e5a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "# YARN ResourceManager URL\n",
    "resource_manager_url = 'http://<resource-manager-host>:8088'\n",
    "\n",
    "# Submit a Hadoop job\n",
    "job_submission_url = f'{resource_manager_url}/ws/v1/cluster/apps'\n",
    "headers = {'Content-Type': 'application/json'}\n",
    "data = {\n",
    "    'application-id': 'application_123456789_0001',\n",
    "    'application-name': 'MyHadoopJob',\n",
    "    'am-container-spec': {\n",
    "        'commands': {\n",
    "            'command': 'hadoop jar my_job.jar input_path output_path'\n",
    "        },\n",
    "        'resource': {\n",
    "            'memory': 2048,  # Specify the desired memory in MB\n",
    "            'vCores': 2      # Specify the desired number of virtual cores\n",
    "        }\n",
    "    },\n",
    "    'application-type': 'MAPREDUCE'\n",
    "}\n",
    "response = requests.post(job_submission_url, headers=headers, json=data)\n",
    "if response.status_code == 202:\n",
    "    print('Hadoop job submitted successfully.')\n",
    "    print('Application ID:', response.json()['application-id'])\n",
    "else:\n",
    "    print('Failed to submit Hadoop job.')\n",
    "    print('Response:', response.text)\n",
    "    exit(1)\n",
    "\n",
    "# Monitor job progress and track resource usage\n",
    "application_id = response.json()['application-id']\n",
    "job_status_url = f'{resource_manager_url}/ws/v1/cluster/apps/{application_id}'\n",
    "while True:\n",
    "    response = requests.get(job_status_url)\n",
    "    if response.status_code == 200:\n",
    "        app_info = response.json()['app']\n",
    "        status = app_info['state']\n",
    "        tracking_url = app_info['trackingUrl']\n",
    "        allocated_resources = app_info['allocatedResources']\n",
    "        used_resources = app_info['usedResources']\n",
    "\n",
    "        if status == 'FINISHED':\n",
    "            print('Hadoop job completed successfully.')\n",
    "            break\n",
    "        elif status == 'FAILED' or status == 'KILLED':\n",
    "            print('Hadoop job failed or killed.')\n",
    "            print('Final status:', status)\n",
    "            exit(1)\n",
    "        else:\n",
    "            print('Job status:', status)\n",
    "            print('Tracking URL:', tracking_url)\n",
    "            print('Allocated Resources:', allocated_resources)\n",
    "            print('Used Resources:', used_resources)\n",
    "    else:\n",
    "        print('Failed to get job status.')\n",
    "        print('Response:', response.text)\n",
    "        exit(1)\n",
    "    time.sleep(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6716757f",
   "metadata": {},
   "outputs": [],
   "source": [
    "9. Write a Python program that compares the performance of a MapReduce job with different input split sizes, showcasing the impact on overall job execution time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9021b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "\n",
    "# Define the Hadoop streaming jar file\n",
    "hadoop_streaming_jar = '/path/to/hadoop-streaming.jar'  # Replace with the actual path to the Hadoop streaming jar file\n",
    "\n",
    "# Define the input file path\n",
    "input_file = '/path/to/input/file'  # Replace with the actual path to the input file\n",
    "\n",
    "# Define the output directory path\n",
    "output_dir = '/path/to/output/directory'  # Replace with the actual path to the output directory\n",
    "\n",
    "# Define the different input split sizes to test\n",
    "split_sizes = [64, 128, 256]  # Replace with the desired split sizes in MB\n",
    "\n",
    "# Run the MapReduce job for each split size\n",
    "for split_size in split_sizes:\n",
    "    # Set the Hadoop configuration for the input split size\n",
    "    config_cmd = ['hadoop', 'jar', hadoop_streaming_jar, '-D', 'mapreduce.input.fileinputformat.split.maxsize=' + str(split_size * 1024 * 1024), '-input', input_file, '-output', output_dir + '_' + str(split_size), '-mapper', 'mapper.py', '-reducer', 'reducer.py', '-file', 'mapper.py', '-file', 'reducer.py']\n",
    "\n",
    "    # Execute the MapReduce job\n",
    "    start_time = time.time()\n",
    "    subprocess.call(config_cmd)\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Calculate and print the execution time\n",
    "    execution_time = end_time - start_time\n",
    "    print(f'Split Size: {split_size} MB, Execution Time: {execution_time} seconds')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
