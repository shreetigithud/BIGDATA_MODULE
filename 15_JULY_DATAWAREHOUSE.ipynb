{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f709bb7d",
   "metadata": {},
   "source": [
    "# TOPIC: Data Warehousing Fundamentals\n",
    "   1. Design a data warehouse schema for a retail company that includes dimension tables for products, customers, and time. Implement the schema using a relational database management system (RDBMS) of your choice.\n",
    "   2. Create a fact table that captures sales data, including product ID, customer ID, date, and sales amount. Populate the fact table with sample data.\n",
    "   3. Write SQL queries to retrieve sales data from the data warehouse, including aggregations and filtering based on different dimensions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c633f12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Create the fact table\n",
    "CREATE TABLE sales (\n",
    "    product_id INT,\n",
    "    customer_id INT,\n",
    "    date_id INT,\n",
    "    sales_amount DECIMAL(10, 2),\n",
    "    units_sold INT,\n",
    "    PRIMARY KEY (product_id, customer_id, date_id),\n",
    "    FOREIGN KEY (product_id) REFERENCES products(product_id),\n",
    "    FOREIGN KEY (customer_id) REFERENCES customers(customer_id),\n",
    "    FOREIGN KEY (date_id) REFERENCES time(date_id)\n",
    ");\n",
    "\n",
    "-- Populate the fact table with sample data\n",
    "INSERT INTO sales (product_id, customer_id, date_id, sales_amount, units_sold)\n",
    "VALUES\n",
    "    (1, 101, 1, 100.00, 5),\n",
    "    (2, 102, 1, 50.00, 2),\n",
    "    (1, 103, 2, 75.00, 3),\n",
    "    (3, 101, 2, 200.00, 8),\n",
    "    ...\n",
    "SELECT t.year, SUM(s.sales_amount) AS total_sales_amount\n",
    "FROM sales s\n",
    "JOIN time t ON s.date_id = t.date_id\n",
    "GROUP BY t.year;\n",
    "\n",
    "SELECT p.category, t.quarter, SUM(s.sales_amount) AS total_sales_amount\n",
    "FROM sales s\n",
    "JOIN products p ON s.product_id = p.product_id\n",
    "JOIN time t ON s.date_id = t.date_id\n",
    "WHERE p.category = 'Electronics' AND t.quarter = 3\n",
    "GROUP BY p.category, t.quarter;\n",
    "\n",
    "SELECT c.city, p.brand, SUM(s.sales_amount) AS total_sales_amount\n",
    "FROM sales s\n",
    "JOIN customers c ON s.customer_id = c.customer_id\n",
    "JOIN products p ON s.product_id = p.product_id\n",
    "GROUP BY c.city, p.brand;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74885299",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd19fa35",
   "metadata": {},
   "source": [
    "# TOPIC: ETL and Data Integration\n",
    "  1. Design an ETL process using a programming language (e.g., Python) to extract data from a source system (e.g., CSV files), transform it by applying certain business rules or calculations, and load it into a data warehouse.\n",
    "  2. Implement the ETL process by writing code that performs the extraction, transformation, and loading steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a913fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import psycopg2  # Assuming PostgreSQL data warehouse\n",
    "\n",
    "# Extraction: Read data from CSV file\n",
    "def extract_data(file_path):\n",
    "    with open(file_path, 'r') as csv_file:\n",
    "        reader = csv.DictReader(csv_file)\n",
    "        return [row for row in reader]\n",
    "\n",
    "# Transformation: Apply business rules and calculations\n",
    "def transform_data(data):\n",
    "    transformed_data = []\n",
    "    for row in data:\n",
    "        # Apply business rules and calculations\n",
    "        transformed_row = {\n",
    "            'column1': row['column1'],\n",
    "            'column2': int(row['column2']),\n",
    "            'column3': float(row['column3']),\n",
    "            'column4': row['column4'].upper(),\n",
    "            # Additional transformations and business rules...\n",
    "        }\n",
    "        transformed_data.append(transformed_row)\n",
    "    return transformed_data\n",
    "\n",
    "# Loading: Load transformed data into data warehouse\n",
    "def load_data(data):\n",
    "    conn = psycopg2.connect(\n",
    "        host=\"your_host\",\n",
    "        port=\"your_port\",\n",
    "        database=\"your_database\",\n",
    "        user=\"your_user\",\n",
    "        password=\"your_password\"\n",
    "    )\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Create or update tables in the data warehouse\n",
    "    create_table_query = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS my_table (\n",
    "            column1 TEXT,\n",
    "            column2 INT,\n",
    "            column3 FLOAT,\n",
    "            column4 TEXT\n",
    "            -- Additional columns...\n",
    "        )\n",
    "    \"\"\"\n",
    "    cursor.execute(create_table_query)\n",
    "\n",
    "    # Load transformed data into the data warehouse\n",
    "    insert_query = \"\"\"\n",
    "        INSERT INTO my_table (column1, column2, column3, column4)\n",
    "        VALUES (%s, %s, %s, %s)\n",
    "    \"\"\"\n",
    "    for row in data:\n",
    "        cursor.execute(insert_query, (row['column1'], row['column2'], row['column3'], row['column4']))\n",
    "\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "# Main ETL Process\n",
    "def etl_process():\n",
    "    # Extract data from CSV files\n",
    "    extracted_data = extract_data('data.csv')\n",
    "\n",
    "    # Transform data\n",
    "    transformed_data = transform_data(extracted_data)\n",
    "\n",
    "    # Load transformed data into the data warehouse\n",
    "    load_data(transformed_data)\n",
    "\n",
    "# Run the ETL process\n",
    "etl_process()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95d5085",
   "metadata": {},
   "source": [
    "# TOPIC: Dimensional Modeling and Schemas\n",
    "   1. Design a star schema for a university database, including a fact table for student enrollments and dimension tables for students, courses, and time. Implement the schema using a database of your choice.\n",
    "   2. Write SQL queries to retrieve data from the star schema, including aggregations and joins between the fact table and dimension tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283f8d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT t.year, c.department, COUNT(*) AS enrollment_count\n",
    "FROM \n",
    "enrollments e\n",
    "JOIN time t ON e.date_id = t.date_id\n",
    "JOIN courses c ON e.course_id = c.course_id\n",
    "GROUP BY t.year, c.department;\n",
    "\n",
    "\n",
    "SELECT e.enrollment_id, s.student_name, c.course_name, t.date\n",
    "FROM enrollments e\n",
    "JOIN students s ON e.student_id = s.student_id\n",
    "JOIN courses c ON e.course_id = c.course_id\n",
    "JOIN time t ON e.date_id = t.date_id\n",
    "WHERE e.enrollment_id = 12345;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bccca4",
   "metadata": {},
   "source": [
    "# TOPIC: Performance Optimization and Querying\n",
    "    1. Scenario: You need to improve the performance of your data loading process in the data warehouse. Write a Python script that implements the following optimizations:\n",
    "Utilize batch processing techniques to load data in bulk instead of individual row insertion.\n",
    "      b)  Implement multi-threading or multiprocessing to parallelize the data loading process.\n",
    "      c)  Measure the time taken to load a specific amount of data before and after implementing these optimizations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356831e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from multiprocessing import Pool\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# Function to load data in bulk using batch processing\n",
    "def load_data_in_bulk(data):\n",
    "    client = MongoClient('mongodb://localhost:27017/')\n",
    "    database = client['mydatabase']\n",
    "    collection = database['mycollection']\n",
    "\n",
    "    # Bulk insert the data\n",
    "    collection.insert_many(data)\n",
    "\n",
    "    client.close()\n",
    "\n",
    "# Function to load data in parallel using multiprocessing\n",
    "def load_data_parallel(data):\n",
    "    client = MongoClient('mongodb://localhost:27017/')\n",
    "    database = client['mydatabase']\n",
    "    collection = database['mycollection']\n",
    "\n",
    "    # Define the number of processes (can be adjusted based on system resources)\n",
    "    num_processes = 4\n",
    "\n",
    "    # Split data into chunks\n",
    "    chunk_size = len(data) // num_processes\n",
    "    data_chunks = [data[i:i + chunk_size] for i in range(0, len(data), chunk_size)]\n",
    "\n",
    "    # Create a pool of worker processes\n",
    "    with Pool(processes=num_processes) as pool:\n",
    "        # Parallelize the data loading process\n",
    "        pool.map(collection.insert_many, data_chunks)\n",
    "\n",
    "    client.close()\n",
    "\n",
    "# Main script\n",
    "def main():\n",
    "    # Generate sample data to load into the data warehouse\n",
    "    data = [\n",
    "        {\"id\": 1, \"name\": \"John Doe\"},\n",
    "        {\"id\": 2, \"name\": \"Jane Smith\"},\n",
    "        {\"id\": 3, \"name\": \"Alice Johnson\"},\n",
    "        # Add more sample data...\n",
    "    ]\n",
    "\n",
    "    # Measure time taken to load data before optimizations\n",
    "    start_time = time.time()\n",
    "    load_data_in_bulk(data)\n",
    "    end_time = time.time()\n",
    "    print(\"Time taken (before optimizations):\", end_time - start_time, \"seconds\")\n",
    "\n",
    "    # Measure time taken to load data after implementing optimizations\n",
    "    start_time = time.time()\n",
    "    load_data_parallel(data)\n",
    "    end_time = time.time()\n",
    "    print(\"Time taken (after optimizations):\", end_time - start_time, \"seconds\")\n",
    "\n",
    "# Run the script\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
